---
layout: default
title: 模型解释性-PDP
---

# 介绍

### 理想定义
PDP(Partial Dependence Plot) 局部依赖图，定义单特征和目标之间的关系，即函数 $\hat{f}_X(x_S)$
$$
\hat{f}_S(x_S)=E_{X_C}[\hat{f}(x_S,X_C)]=\int{\hat{f}(x_S,X_C)dP(X_C)}
$$
$x_S$ 标识当前特征 $S$ 的特征值，$X_C$ 表示出了 $S$ 以外的其他特征，$E_{X_C}$ 表示给定 $X_S$ 时，模型 $\hat{f}$ 对特征 $X_C$ 的期望。

### 约束在特定的数据集上
求期望需要枚举 $X_C$，但枚举所有值算力消耗过高，而且某些特征取值的组合不一定有意义，因此取特定数据集中的特征值，模型
$$
\hat{f}_S(X_S)=\frac{1}{n}\sum_{i=1}^n{\hat{f}(x_S, X_C^{(i)})}
$$
当分析的单特征类型为类别型特征，直接看不同取值即可。
至此，就可以根据PDP对模型的单个特征做出人为可理解的解释。

### 单值衡量特征重要性
有了PDP的特征-结果关系函数 $\hat{y}(x)$ 之后，对数值型特征，可以将方差作为特征重要性的衡量。
对类别型特征呢？可以将 $\frac{max(f(x)) - min(f(x))}{4}$ 作为方差的近似。因为在正太分布中，95%的数据都满足  $x {\pm} 2 * var = mean$

# 优点
直观
容易应用
适用于任何黑盒模型

# 缺点
PDP只能做 <=2 个特征的解释，严格说这不是方法的缺点，而是人类的理解能力只能在 <=2 维度上
PDP依赖数据分布，可以加个散点图来解决
有特征独立的假设
某些影响可能会被隐藏：多类数据特征解释性相反，合并在一起就会变现为特征没有作用。其实和特征独立的假设是一个问题。
# 应用
`scikit-learn.PDPBox`
# 参考
https://christophm.github.io/interpretable-ml-book/pdp.html