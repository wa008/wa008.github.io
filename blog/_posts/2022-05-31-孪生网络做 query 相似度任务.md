---
layout: default
title: 孪生网络做 query 相似度任务
---

# 起因
工作中做一个 query 相似度任务时，偶然看到孪生网络的一片经典论文，[《Learning Text Similarity with Siamese Recurrent Networks》](https://aclanthology.org/W16-1617.pdf)，用来做同类文本预料的相似度任务是极好的。

# 介绍
这篇文论主介绍孪生网络的基本思想，对于 query 相似度任务（同类型实体的相似度任务，比如图片、语音），可以设计一个网络结构 net，将两个 query 都经过相同的 net 投影到对应的空间，再该空间中对 net(x) 和 net(y) 求取相似度， net(x) 可以看到 x 在高纬空间的表征。
原 paper 中用 BiLSTM 网络结构来表征 query，模型结构如下：
![网络架构](http://informal.top/usr/uploads/2021/12/835350353.png)

# 实验
数据取自百度千言的文本相似度评测数据 bq_corpus [数据集](https://aistudio.baidu.com/aistudio/competition/detail/45/0/submit-result)。
该论文的 loss 比较像 MSE 的变种，判定正负例的阈值对超参 m 有明显影响，从试验结论看该值设置的越小越好。
![26015-zfotz81y52a.png](http://informal.top/usr/uploads/2021/12/2662121877.png)

-|-|-|-|-
| 版本               | 训练集acc  | 验证集acc | 说明 
| ----              | ---       | --------- | ----|
| bert              | 0.979     | 0.841     | bert基准版本 |
| mse m = 0.4       |0.60       |0.59     |原loss | 
| mse m = 0          |0.723     |0.724    |降低超参m比较有用|
| mse m = 0 学习率3e-4|0.81      |0.758      |调整学习率，最优版本|
| norm + cos        | 0.722     | 0.721     | norm 没啥效果|
| BCEloss             | 0.637   |0.641     |修改loss效果不大|
-|-|-|-|-

以上 acc 的预测阈值为 0.5，即预测值 > 0.5 算正例，否则算负例。
小结论如下：
1. 从实验数据看，需要根据预测阈值，来调整一个合适的超参 m，本文是越小越好。
1. 设置合适的学习率，参考别人的结论设置 3e-4 较好，本文没有试验很多。
1. 做cos之前添加 norm，没有明显提升，猜测是与具体的语料相关。
1. 修改 loss 为 -log 损失函数，也没有明显提升，难道不应该梯度越大，收敛的越好吗？这个很奇怪。
1. 孪生网络与 bert 验证集相差 9 个点左右。

# 结论
孪生网络不同于 bert 把两个 query 合并在一起求二分类任务，定义了另一种解决相似度任务的方式。虽然从时间上看孪生网络的提出时间要更早一些。
另一个角度看孪生网络，更像是把一个构造 query 向量的任务，可以从这个角度寻找更多有意思的解法。

# 进阶
很多用 CNN 来做 query 理解的，很有意思，值得学习下

# 参考
[1]、https://aclanthology.org/W16-1617.pdf
[2]、https://mp.weixin.qq.com/s/UDG5z4lcOiRquRN0H6ELCQ
