{
  "by" : "zhisbug",
  "descendants" : 52,
  "id" : 40302201,
  "kids" : [ 40303428, 40303071, 40303379, 40302689, 40303845, 40303311, 40303926, 40302569, 40302564, 40302970, 40303128, 40303450, 40302584, 40303724, 40303122, 40303643 ],
  "score" : 229,
  "time" : 1715198107,
  "title" : "Consistency LLM: converting LLMs to parallel decoders accelerates inference 3.5x",
  "type" : "story",
  "url" : "https://hao-ai-lab.github.io/blogs/cllm/"
}
