{
  "by" : "rushingcreek",
  "descendants" : 78,
  "id" : 38088538,
  "kids" : [ 38089763, 38089772, 38089222, 38089573, 38089565, 38089281, 38089104, 38089704, 38089674, 38089682, 38089329, 38089246, 38089739, 38089266, 38089455, 38089312, 38089333, 38088995, 38089290, 38089475, 38089606, 38089173, 38089214, 38089683, 38089275, 38089453, 38089384, 38089131, 38089321, 38089205, 38089263, 38089018, 38088893 ],
  "score" : 159,
  "text" : "Hi HN,<p>We’re excited to announce that Phind now defaults to our own model that matches and exceeds GPT-4’s coding abilities while running 5x faster. You can now get high quality answers for technical questions in 10 seconds instead of 50.<p>The current 7th-generation Phind Model is built on top of our open-source CodeLlama-34B fine-tunes that were the first models to beat GPT-4’s score on HumanEval and are still the best open source coding models overall by a wide margin: <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;bigcode&#x2F;bigcode-models-leaderboard\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;bigcode&#x2F;bigcode-models-leaderb...</a>.<p>This new model has been fine-tuned on an additional 70B+ tokens of high quality code and reasoning problems and exhibits a HumanEval score of 74.7%. However, we’ve found that HumanEval is a poor indicator of real-world helpfulness. After deploying previous iterations of the Phind Model on our service, we’ve collected detailed feedback and noticed that our model matches or exceeds GPT-4’s helpfulness most of the time on real-world questions. Many in our Discord community have begun using Phind exclusively with the Phind Model despite also having unlimited access to GPT-4.<p>One of the Phind Model’s key advantages is that it&#x27;s very fast. We’ve been able to achieve a 5x speedup over GPT-4 by running our model on H100s using the new TensorRT-LLM library from NVIDIA. We can achieve up to 100 tokens per second single-stream while GPT-4 runs around 20 tokens per second at best.<p>Another key advantage of the Phind Model is context – it supports up to 16k tokens. We currently allow inputs of up to 12k tokens on the website and reserve the remaining 4k for web results.<p>There are still some rough edges with the Phind Model and we’ll continue improving it constantly. One area where it still suffers is consistency — on certain challenging questions where it is capable of getting the right answer, the Phind Model might take more generations to get to the right answer than GPT-4.<p>We’d love to hear your feedback.<p>Cheers,<p>The Phind Team",
  "time" : 1698774047,
  "title" : "Show HN: Phind Model beats GPT-4 at coding, with GPT-3.5 speed and 16k context",
  "type" : "story",
  "url" : "https://www.phind.com/phindmodelhn"
}
