{
  "by" : "danielhanchen",
  "descendants" : 7,
  "id" : 38487199,
  "kids" : [ 38493724, 38494191, 38494918, 38494949, 38494555, 38487200 ],
  "score" : 36,
  "text" : "Hi HN! I&#x27;m just sharing a project I&#x27;ve been working on during the LLM Efficiency Challenge - you can now finetune Llama with QLoRA 5x faster than Huggingface&#x27;s original implementation on your own local GPU. Some highlights:<p>1. Manual autograd engine - hand derived backprop steps.<p>2. QLoRA &#x2F; LoRA 80% faster, 50% less memory.<p>3. All kernels written in OpenAI&#x27;s Triton language.<p>4. 0% loss in accuracy - no approximation methods - all exact.<p>5. No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+.<p>6. Flash Attention support via Xformers.<p>7. Supports 4bit and 16bit LoRA finetuning.<p>8. Train Slim Orca fully locally in 260 hours from 1301 hours (5x faster).<p>9. Open source version trains 5x faster or you can check out Unsloth Pro and Max codepaths for 30x faster training!<p><a href=\"https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;188197j&#x2F;80_faster_50_less_memory_0_accuracy_loss_llama&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;188197j&#x2F;80_fast...</a> has more info about Unsloth!<p>Hopefully you can try it out! Wrote a blog post at <a href=\"https:&#x2F;&#x2F;unsloth.ai&#x2F;introducing\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;unsloth.ai&#x2F;introducing</a> if you want to learn more about our manual hand derived backprop or Triton kernels and stuff! Thanks once again!",
  "time" : 1701441760,
  "title" : "Show HN: 80% faster, 50% less memory, 0% loss of accuracy Llama finetuning",
  "type" : "story",
  "url" : "https://github.com/unslothai/unsloth"
}
