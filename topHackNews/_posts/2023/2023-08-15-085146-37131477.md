{
  "by" : "michellezzz",
  "descendants" : 0,
  "id" : 37131477,
  "score" : 11,
  "time" : 1692087671,
  "title" : "Continuous batch enables 23x throughput in LLM inference and reduce p50 latency",
  "type" : "story",
  "url" : "https://www.anyscale.com/blog/continuous-batching-llm-inference"
}
